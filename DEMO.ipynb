{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of `torch_focalloss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import float32, ones, randint, randn, tensor\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "\n",
    "from torch_focalloss import BinaryFocalLoss, MultiClassFocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BinaryFocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BinaryFocalLoss for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the same inputs for the whole example to demonstrate how changes in parameters changes the loss value.\n",
    "\n",
    "First we create our simulated batch of 5 binary labels and raw logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:  tensor([-1.7464, -0.3476,  2.7578, -0.1100,  0.5949])\n",
      "Target:  tensor([1., 1., 1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "preds = randn(5)\n",
    "target = randint(2, size=(5,), dtype=float32)\n",
    "print(\"Logits: \", preds)\n",
    "print(\"Target: \", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal binary cross entropy loss is the same as focal loss when $\\gamma$ (gamma), which determines the strength of focus on difficult samples, is equal to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE Loss: 0.78592\n",
      "Focal Loss: 0.78592\n"
     ]
    }
   ],
   "source": [
    "gamma = 0\n",
    "\n",
    "bce = BCEWithLogitsLoss()\n",
    "bfl = BinaryFocalLoss(gamma=gamma)\n",
    "\n",
    "print(f\"BCE Loss: {bce(preds, target).item():.5f}\")\n",
    "print(f\"Focal Loss: {bfl(preds, target).item():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also true when the weight applied to the positive class (1) relative to the negative class (0) is not 1. This parameter is called $\\alpha$ (alpha) and is identical to the `pos_weight` parameter of the `BCEWithLogits` class, which is used to help manage class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE Loss: 1.11491\n",
      "Focal Loss: 1.11491\n"
     ]
    }
   ],
   "source": [
    "gamma = 0\n",
    "alpha = 1.5\n",
    "\n",
    "bce = BCEWithLogitsLoss(pos_weight=tensor(alpha))\n",
    "bfl = BinaryFocalLoss(gamma=gamma, alpha=alpha)\n",
    "\n",
    "print(f\"BCE Loss: {bce(preds, target).item():.5f}\")\n",
    "print(f\"Focal Loss: {bfl(preds, target).item():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our $\\alpha$ is similar, but not identical, to the one in Lin et al.'s \"Focal Loss for Dense Object Detection\" (https://arxiv.org/abs/1708.02002). Both implementations use $\\alpha$ as the weight for the positive class, but Lin et al. uses $(1-\\alpha)$ as the weight for the negative class, whereas our implementation implicitly uses $1$ as the weight for the negative class. This means that Lin et al.'s $\\alpha$ is constrained to $[0,1]$, but ours is unbounded.\n",
    "\n",
    "The formula $\\alpha = L / (1-L)$, where $L$ is the $\\alpha$ from Lin et al., converts between the two. However, to eliminate balancing and replicate the behavior for $\\alpha=1$ using the Lin et al. implementation, we must set $L=0.5$ and multiply the final loss by 2, which demonstrates that the conversion is not 1-to-1 when it comes to training behavior. Notably, it requires reevaluation of the learning rate in particular, generally requiring lower learning rates in our implementation compared to Lin et al. for $\\alpha>1$ and higher learning rates for $\\alpha<1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal loss differs from binary cross entropy loss when $\\gamma\\neq0$. Technically, $\\gamma$ can be less than $0$, but this would increase focus on easy samples and defocus hard samples, which is the opposite of why focal loss is effective. Thus, we will show what happens when $\\gamma>0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE Loss: 0.78592\n",
      "Focal Loss: 0.37685\n"
     ]
    }
   ],
   "source": [
    "gamma = 2\n",
    "\n",
    "bce = BCEWithLogitsLoss()\n",
    "bfl = BinaryFocalLoss(gamma=gamma)\n",
    "\n",
    "print(f\"BCE Loss: {bce(preds, target).item():.5f}\")\n",
    "print(f\"Focal Loss: {bfl(preds, target).item():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BinaryFocalLoss for multi-label classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like binary cross entropy loss, we can use our binary focal loss for multi-label classification without modification.\n",
    "\n",
    "We will simulate a batch of 5 samples, each with 3 binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: \n",
      " tensor([[ 1.4674,  0.1618,  0.6060],\n",
      "        [ 0.1765,  0.7799,  0.9048],\n",
      "        [-0.5558,  1.5306, -0.4360],\n",
      "        [ 0.8222,  0.0072,  0.7803],\n",
      "        [ 1.1644,  0.3844,  0.4152]])\n",
      "Target: \n",
      " tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "preds = randn(5, 3)\n",
    "target = randint(2, size=(5, 3), dtype=float32)\n",
    "print(\"Logits: \\n\", preds)\n",
    "print(\"Target: \\n\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE Loss: 0.85098\n",
      "Focal Loss: 0.28560\n"
     ]
    }
   ],
   "source": [
    "gamma = 2\n",
    "alpha = 1.5\n",
    "\n",
    "bce = BCEWithLogitsLoss(pos_weight=tensor(alpha))\n",
    "bfl = BinaryFocalLoss(gamma=gamma, alpha=alpha)\n",
    "\n",
    "print(f\"BCE Loss: {bce(preds, target).item():.5f}\")\n",
    "print(f\"Focal Loss: {bfl(preds, target).item():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing multi-label classification, you can also specify a value of $\\alpha$ for each label by combining them in a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE Loss: 0.74597\n",
      "Focal Loss: 0.27082\n"
     ]
    }
   ],
   "source": [
    "gamma = 2\n",
    "alpha = tensor([0.5, 1, 1.5])\n",
    "\n",
    "bce = BCEWithLogitsLoss(pos_weight=alpha)\n",
    "bfl = BinaryFocalLoss(gamma=gamma, alpha=alpha)\n",
    "\n",
    "print(f\"BCE Loss: {bce(preds, target).item():.5f}\")\n",
    "print(f\"Focal Loss: {bfl(preds, target).item():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiClassFocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also extended Lin et al.'s focal loss, which they only defined for the binary case, to the multiclass case.\n",
    "\n",
    "Our example input will be for a 4-class classification problem, so we will create a sample of 5 labels and 5 sets of logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: \n",
      " tensor([[-0.4086,  0.0477,  0.6028,  0.4822],\n",
      "        [-0.7605, -0.2642,  0.2710,  1.3920],\n",
      "        [-0.8921, -1.3226,  0.0824, -1.7038],\n",
      "        [ 0.1170,  0.9550, -1.3464,  0.5218],\n",
      "        [ 1.5800,  0.2870,  0.2940, -1.0333]])\n",
      "Target: \n",
      " tensor([1, 2, 3, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "preds = randn(5, 4)\n",
    "target = randint(4, size=(5,))\n",
    "print(\"Logits: \\n\", preds)\n",
    "print(\"Target: \\n\", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like binary focal loss and binary cross entropy loss, multi-class focal loss and cross entropy loss are the same when $\\gamma=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss: 1.79243\n",
      "Multi-Class Focal Loss: 1.79243\n"
     ]
    }
   ],
   "source": [
    "gamma = 0\n",
    "\n",
    "cel = CrossEntropyLoss()\n",
    "mcfl = MultiClassFocalLoss(gamma=gamma)\n",
    "\n",
    "print(f\"Cross Entropy Loss: {cel(preds, target).item():.5f}\")\n",
    "print(f\"Multi-Class Focal Loss: {mcfl(preds, target).item():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also true when we apply class balancing weights. We also call these $\\alpha$, and they are identical to the \"weight\" argument of the `CrossEntropyLoss` class. Note that when using the reduction option `\"mean\"`, the weighted mean is taken, which means that the sum is divided by the effective number of samples according to the class weights. This is the same behavior as for the standard `CrossEntropyLoss` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: tensor([0.3806, 2.1139, 0.1361, 2.1312])\n",
      "\n",
      "Cross Entropy Loss: 1.93800\n",
      "Multi-Class Focal Loss: 1.93800\n"
     ]
    }
   ],
   "source": [
    "gamma = 0\n",
    "alpha = (ones(4) + randn(4)).abs()\n",
    "print(f\"Alpha: {alpha}\\n\")\n",
    "\n",
    "cel = CrossEntropyLoss(weight=alpha)\n",
    "mcfl = MultiClassFocalLoss(gamma=gamma, alpha=alpha)\n",
    "\n",
    "print(f\"Cross Entropy Loss: {cel(preds, target).item():.5f}\")\n",
    "print(f\"Multi-Class Focal Loss: {mcfl(preds, target).item():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the binary case, multi-class focal loss differs from cross entropy loss when $\\gamma\\neq0$. Again, we will only show what happens when $\\gamma>0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss: 1.93800\n",
      "Multi-Class Focal Loss: 1.42661\n"
     ]
    }
   ],
   "source": [
    "gamma = 2\n",
    "\n",
    "cel = CrossEntropyLoss(weight=alpha)\n",
    "mcfl = MultiClassFocalLoss(gamma=gamma, alpha=alpha)\n",
    "\n",
    "print(f\"Cross Entropy Loss: {cel(preds, target).item():.5f}\")\n",
    "print(f\"Multi-Class Focal Loss: {mcfl(preds, target).item():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for checking out the `torch_focalloss` package!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testbuild",
   "language": "python",
   "name": "testbuild"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
